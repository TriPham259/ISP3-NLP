{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_raw_df = fetch_20newsgroups(subset='train', data_home='./scikit_learn_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting word occurrence. The reason behind of using this approach is that keyword or important signal will occur again and again. So if the number of occurrence represent the importance of word. More frequency means more importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word  Count\n",
       "16   of      3\n",
       "26  the      3\n",
       "3   bow      2\n",
       "0   and      1\n",
       "28  way      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\"\n",
    "\n",
    "# Initialize a CountVectorizer object\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "# Transforms the data into a bag of words (sparse matrix)\n",
    "count_occurs = count_vec.fit_transform([doc])\n",
    "\n",
    "# Create a table to count occurrence of each word\n",
    "count_occur_df = pd.DataFrame((count, word) for word, count in \n",
    "                              zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
    "count_occur_df.columns = ['Word', 'Count']\n",
    "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "count_occur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Count Occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization can be applied to avoid model bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word     Count\n",
       "16   of  0.428571\n",
       "26  the  0.428571\n",
       "3   bow  0.285714\n",
       "0   and  0.142857\n",
       "28  way  0.142857"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\"\n",
    "\n",
    "# Initialize a TfidfVectorizer object\n",
    "norm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "\n",
    "# Transforms the data into a bag of words (sparse matrix)\n",
    "norm_count_occurs = norm_count_vec.fit_transform([doc])\n",
    "\n",
    "# Create a table to count occurrence of each word\n",
    "norm_count_occur_df = pd.DataFrame((count, word) for word, count in \n",
    "                                   zip(norm_count_occurs.toarray().tolist()[0], norm_count_vec.get_feature_names()))\n",
    "norm_count_occur_df.columns = ['Word', 'Count']\n",
    "norm_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "norm_count_occur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF take another approach which is believe that high frequency may not able to provide much information gain. In another word, rare words may contribute more weights to the model. \n",
    "Word importance will be increased if the word occurs less frequently in the document (i.e. training record)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word     Count\n",
       "16   of  0.428571\n",
       "26  the  0.428571\n",
       "3   bow  0.285714\n",
       "0   and  0.142857\n",
       "28  way  0.142857"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\"\n",
    "\n",
    "# Initialize a TfidfVectorizer object\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "\n",
    "# Transforms the data into a bag of words (sparse matrix)\n",
    "tfidf_count_occurs = tfidf_vec.fit_transform([doc])\n",
    "\n",
    "# Create a table to count occurrence of each word\n",
    "tfidf_count_occur_df = pd.DataFrame((count, word) for word, count in \n",
    "                                    zip(tfidf_count_occurs.toarray().tolist()[0], tfidf_vec.get_feature_names()))\n",
    "tfidf_count_occur_df.columns = ['Word', 'Count']\n",
    "tfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "tfidf_count_occur_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of irrelevant words\n",
    "stop_words = ['a', 'an', 'the']\n",
    "\n",
    "# cleaning one element of the corpus\n",
    "def cleaning(text):\n",
    "    # split text into a list of words\n",
    "    tokens = text.split(' ')\n",
    "    # change all letters to lower case\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # Remove stop words\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    # return cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# add all cleaned text in to corpus\n",
    "def preprocess_x(x):\n",
    "    processed_x = [cleaning(text) for text in x]\n",
    "    \n",
    "    return processed_x\n",
    "\n",
    "# build bag of words\n",
    "def build_model(mode):\n",
    "    vect = None\n",
    "    if mode == 'count':\n",
    "        vect = CountVectorizer()\n",
    "    elif mode == 'tf':\n",
    "        vect = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    elif mode == 'tfidf':\n",
    "        vect = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError('Mode should be either count or tfidf')\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "def pipeline(x, y, mode):\n",
    "    processed_x = preprocess_x(x)\n",
    "    \n",
    "    model_pipeline = build_model(mode)\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Vocabulary: 130107\n"
     ]
    }
   ],
   "source": [
    "x = preprocess_x(x_train)\n",
    "y = y_train\n",
    "    \n",
    "model_pipeline = build_model(mode='count')\n",
    "model_pipeline.fit(x, y)\n",
    "\n",
    "print('Number of Vocabulary: %d'% (len(model_pipeline.named_steps['vect'].get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer------\n",
      "Accuracy: 0.8911 (+/- 0.0091)\n",
      "Using TF Vectorizer------\n",
      "Accuracy: 0.8035 (+/- 0.0115)\n",
      "Using TF-IDF Vectorizer------\n",
      "Accuracy: 0.8915 (+/- 0.0084)\n"
     ]
    }
   ],
   "source": [
    "print('Using Count Vectorizer------')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='count')\n",
    "\n",
    "print('Using TF Vectorizer------')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='tf')\n",
    "\n",
    "print('Using TF-IDF Vectorizer------')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
