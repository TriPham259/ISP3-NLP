{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import word2vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_raw_df = fetch_20newsgroups(subset='train', data_home='./scikit_learn_data')\n",
    "x_train = train_raw_df.data\n",
    "y_train = train_raw_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write the required functions to process our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /usr/local/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# library for cleaning texts\n",
    "import re\n",
    "\n",
    "# download the list of all non-significant/irrelevant words\n",
    "import nltk\n",
    "nltk.download('stopwords', download_dir='/usr/local/share/nltk_data')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for stemming (filter out the base form of the words)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# cleaning one sentence of the dataset\n",
    "def cleaning(sentence):\n",
    "    # replace all characters except letters in dataset with ' '\n",
    "    tokens = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # change all letters to lower case\n",
    "    tokens = tokens.lower()\n",
    "\n",
    "    # split the sentence into a list of words\n",
    "    tokens = tokens.split()\n",
    "    \n",
    "    # remove irrelevant words\n",
    "    ps = PorterStemmer()\n",
    "    tokens = [ps.stem(word) for word in tokens if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # return the sentence as a list of words\n",
    "    return tokens\n",
    "\n",
    "# create a list of lists containing words from each sentence\n",
    "def preprocess_wv(x):\n",
    "    corpus = [cleaning(sentence) for sentence in x]   \n",
    "    return corpus\n",
    "\n",
    "# create a corpus of cleaned sentences for BoW models\n",
    "def preprocess_bow(x):\n",
    "    corpus = [' '.join(cleaning(sentence)) for sentence in x]   \n",
    "    return corpus\n",
    "\n",
    "# build BoW models from the corpus\n",
    "def build_model(mode):\n",
    "    # choose which type of model to use\n",
    "    vect = None\n",
    "    if mode == 'count':\n",
    "        vect = CountVectorizer()\n",
    "    elif mode == 'tf':\n",
    "        vect = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    else:\n",
    "        raise ValueError('Mode should be either count or tfidf')\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "# process our dataset\n",
    "def pipeline(x, y, mode):\n",
    "    processed_x = preprocess_bow(x)\n",
    "    \n",
    "    model_pipeline = build_model(mode)\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the vocabulary and train the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38593168, 159920640)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = preprocess_wv(x_train)\n",
    "model = word2vec.Word2Vec(documents, size=100, window=10, min_count=2, workers=4)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vocabulary of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': <gensim.models.keyedvectors.Vocab at 0x1a28c07390>,\n",
       " 'r': <gensim.models.keyedvectors.Vocab at 0x1a28c07f98>,\n",
       " 'o': <gensim.models.keyedvectors.Vocab at 0x1a28c07940>,\n",
       " 'm': <gensim.models.keyedvectors.Vocab at 0x1a28c070b8>,\n",
       " ':': <gensim.models.keyedvectors.Vocab at 0x1a28c079b0>,\n",
       " ' ': <gensim.models.keyedvectors.Vocab at 0x1a28c07550>,\n",
       " 'l': <gensim.models.keyedvectors.Vocab at 0x1a28c07d30>,\n",
       " 'e': <gensim.models.keyedvectors.Vocab at 0x1a28c07f60>,\n",
       " 'x': <gensim.models.keyedvectors.Vocab at 0x1a28c07588>,\n",
       " 's': <gensim.models.keyedvectors.Vocab at 0x1a28c072b0>,\n",
       " 't': <gensim.models.keyedvectors.Vocab at 0x1a28c07080>,\n",
       " '@': <gensim.models.keyedvectors.Vocab at 0x1a28c07470>,\n",
       " 'w': <gensim.models.keyedvectors.Vocab at 0x1a28c07400>,\n",
       " 'a': <gensim.models.keyedvectors.Vocab at 0x1a28c07748>,\n",
       " '.': <gensim.models.keyedvectors.Vocab at 0x1a28c07048>,\n",
       " 'u': <gensim.models.keyedvectors.Vocab at 0x1a28c079e8>,\n",
       " 'd': <gensim.models.keyedvectors.Vocab at 0x1a28c07668>,\n",
       " '(': <gensim.models.keyedvectors.Vocab at 0x1a28c07cf8>,\n",
       " 'h': <gensim.models.keyedvectors.Vocab at 0x1a28c07518>,\n",
       " \"'\": <gensim.models.keyedvectors.Vocab at 0x1a2ab37940>,\n",
       " 'i': <gensim.models.keyedvectors.Vocab at 0x1a2ab37f28>,\n",
       " 'n': <gensim.models.keyedvectors.Vocab at 0x1a2ab37160>,\n",
       " 'g': <gensim.models.keyedvectors.Vocab at 0x1a2ab379e8>,\n",
       " ')': <gensim.models.keyedvectors.Vocab at 0x1a2ab37278>,\n",
       " 'b': <gensim.models.keyedvectors.Vocab at 0x1a2ab37320>,\n",
       " 'j': <gensim.models.keyedvectors.Vocab at 0x1a2ab37be0>,\n",
       " 'c': <gensim.models.keyedvectors.Vocab at 0x1a2ab37588>,\n",
       " '!': <gensim.models.keyedvectors.Vocab at 0x1a2ab37470>,\n",
       " '?': <gensim.models.keyedvectors.Vocab at 0x1a2ab37ac8>,\n",
       " 'p': <gensim.models.keyedvectors.Vocab at 0x1a2ab37438>,\n",
       " '-': <gensim.models.keyedvectors.Vocab at 0x1a2ab37080>,\n",
       " '3': <gensim.models.keyedvectors.Vocab at 0x1a2ab379b0>,\n",
       " 'z': <gensim.models.keyedvectors.Vocab at 0x1a2ab377f0>,\n",
       " 'v': <gensim.models.keyedvectors.Vocab at 0x1a2ab372e8>,\n",
       " 'y': <gensim.models.keyedvectors.Vocab at 0x1a2ab37a20>,\n",
       " ',': <gensim.models.keyedvectors.Vocab at 0x1a2ab37f60>,\n",
       " 'k': <gensim.models.keyedvectors.Vocab at 0x1a2ab37978>,\n",
       " '1': <gensim.models.keyedvectors.Vocab at 0x1a2ab377b8>,\n",
       " '5': <gensim.models.keyedvectors.Vocab at 0x1a2ab37358>,\n",
       " 'I': <gensim.models.keyedvectors.Vocab at 0x1a2ab37c18>,\n",
       " '2': <gensim.models.keyedvectors.Vocab at 0x1a2ab370b8>,\n",
       " '6': <gensim.models.keyedvectors.Vocab at 0x1a2ab37a58>,\n",
       " '0': <gensim.models.keyedvectors.Vocab at 0x1a2ab37f98>,\n",
       " '/': <gensim.models.keyedvectors.Vocab at 0x1a2ab37cf8>,\n",
       " '7': <gensim.models.keyedvectors.Vocab at 0x1a2ab374e0>,\n",
       " 'L': <gensim.models.keyedvectors.Vocab at 0x1a1b30ae48>,\n",
       " 'S': <gensim.models.keyedvectors.Vocab at 0x1a1b30a710>,\n",
       " 'q': <gensim.models.keyedvectors.Vocab at 0x1a1b30a828>,\n",
       " '9': <gensim.models.keyedvectors.Vocab at 0x1a1b30a668>,\n",
       " 'A': <gensim.models.keyedvectors.Vocab at 0x104242ba8>,\n",
       " '8': <gensim.models.keyedvectors.Vocab at 0x1a2c4de978>,\n",
       " '4': <gensim.models.keyedvectors.Vocab at 0x1a2c4de898>,\n",
       " '<': <gensim.models.keyedvectors.Vocab at 0x1a2c4ded30>,\n",
       " '>': <gensim.models.keyedvectors.Vocab at 0x1a2c4de710>,\n",
       " 'E': <gensim.models.keyedvectors.Vocab at 0x1a2c4de1d0>,\n",
       " 'P': <gensim.models.keyedvectors.Vocab at 0x1a2c4de128>,\n",
       " 'B': <gensim.models.keyedvectors.Vocab at 0x1a2c4de7f0>,\n",
       " '*': <gensim.models.keyedvectors.Vocab at 0x1a2c4deac8>,\n",
       " '\"': <gensim.models.keyedvectors.Vocab at 0x1a2c4dea90>,\n",
       " ';': <gensim.models.keyedvectors.Vocab at 0x1a2c4de8d0>,\n",
       " '\\\\': <gensim.models.keyedvectors.Vocab at 0x1a2c4de5f8>,\n",
       " 'F': <gensim.models.keyedvectors.Vocab at 0x1a2c4dee10>,\n",
       " 'W': <gensim.models.keyedvectors.Vocab at 0x1a2c4dee48>,\n",
       " '[': <gensim.models.keyedvectors.Vocab at 0x1a2c4de588>,\n",
       " ']': <gensim.models.keyedvectors.Vocab at 0x1a2c4defd0>,\n",
       " 'D': <gensim.models.keyedvectors.Vocab at 0x1a2c4de320>,\n",
       " '&': <gensim.models.keyedvectors.Vocab at 0x1a2c4def60>,\n",
       " 'M': <gensim.models.keyedvectors.Vocab at 0x1a2c4def28>,\n",
       " '$': <gensim.models.keyedvectors.Vocab at 0x1a2c4dea20>,\n",
       " 'O': <gensim.models.keyedvectors.Vocab at 0x1a2c4deeb8>,\n",
       " 'U': <gensim.models.keyedvectors.Vocab at 0x1a2c4deb00>,\n",
       " 'N': <gensim.models.keyedvectors.Vocab at 0x1a2c4de6a0>,\n",
       " 'C': <gensim.models.keyedvectors.Vocab at 0x1a2c4de630>,\n",
       " '%': <gensim.models.keyedvectors.Vocab at 0x1a2c4dee80>,\n",
       " '{': <gensim.models.keyedvectors.Vocab at 0x1a2c4deba8>,\n",
       " '}': <gensim.models.keyedvectors.Vocab at 0x1a2c4de4e0>,\n",
       " '#': <gensim.models.keyedvectors.Vocab at 0x1a2c4dec88>,\n",
       " 'T': <gensim.models.keyedvectors.Vocab at 0x1a2c4de5c0>,\n",
       " 'X': <gensim.models.keyedvectors.Vocab at 0x1a2c4de3c8>,\n",
       " 'H': <gensim.models.keyedvectors.Vocab at 0x1a2c4de748>,\n",
       " '`': <gensim.models.keyedvectors.Vocab at 0x1a2c4deb70>,\n",
       " '_': <gensim.models.keyedvectors.Vocab at 0x1a2c4de048>,\n",
       " '~': <gensim.models.keyedvectors.Vocab at 0x1a2c4deda0>,\n",
       " 'Y': <gensim.models.keyedvectors.Vocab at 0x1a2c4de4a8>,\n",
       " '=': <gensim.models.keyedvectors.Vocab at 0x1a2c4de828>,\n",
       " '+': <gensim.models.keyedvectors.Vocab at 0x1a2c4de0b8>,\n",
       " 'G': <gensim.models.keyedvectors.Vocab at 0x1a2c4de940>,\n",
       " '|': <gensim.models.keyedvectors.Vocab at 0x1a2c4de550>,\n",
       " 'V': <gensim.models.keyedvectors.Vocab at 0x1a2c4dedd8>,\n",
       " '^': <gensim.models.keyedvectors.Vocab at 0x1a2c4de278>,\n",
       " 'R': <gensim.models.keyedvectors.Vocab at 0x1a2c4de668>,\n",
       " 'J': <gensim.models.keyedvectors.Vocab at 0x1a2c4de438>,\n",
       " 'K': <gensim.models.keyedvectors.Vocab at 0x1a2c4dec50>,\n",
       " 'Z': <gensim.models.keyedvectors.Vocab at 0x1a2c4dec18>,\n",
       " 'é': <gensim.models.keyedvectors.Vocab at 0x1a2c4de470>,\n",
       " 'Q': <gensim.models.keyedvectors.Vocab at 0x1a2c4ded68>,\n",
       " 'ÿ': <gensim.models.keyedvectors.Vocab at 0x1a2c4de240>,\n",
       " '\\x08': <gensim.models.keyedvectors.Vocab at 0x1a2c4def98>,\n",
       " 'þ': <gensim.models.keyedvectors.Vocab at 0x1a2c4de6d8>,\n",
       " 'º': <gensim.models.keyedvectors.Vocab at 0x1a2c4de400>,\n",
       " 'ý': <gensim.models.keyedvectors.Vocab at 0x1a2c4de080>,\n",
       " 'û': <gensim.models.keyedvectors.Vocab at 0x1a2c4de390>,\n",
       " 'ß': <gensim.models.keyedvectors.Vocab at 0x1a2c4decc0>,\n",
       " '÷': <gensim.models.keyedvectors.Vocab at 0x1a2c4deef0>,\n",
       " '³': <gensim.models.keyedvectors.Vocab at 0x1a16ea3e48>,\n",
       " 'ä': <gensim.models.keyedvectors.Vocab at 0x1a1bdd4b70>,\n",
       " 'ª': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad080>,\n",
       " '«': <gensim.models.keyedvectors.Vocab at 0x1a1c7adba8>,\n",
       " '§': <gensim.models.keyedvectors.Vocab at 0x1a1c7ade80>,\n",
       " '»': <gensim.models.keyedvectors.Vocab at 0x1a1c7adb70>,\n",
       " 'ñ': <gensim.models.keyedvectors.Vocab at 0x1a1c7adeb8>,\n",
       " '¹': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad240>,\n",
       " '\\x03': <gensim.models.keyedvectors.Vocab at 0x1a1c7addd8>,\n",
       " '\\x1b': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad0f0>,\n",
       " '\\x1a': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad860>,\n",
       " 'Ñ': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad898>,\n",
       " '\\x7f': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad320>,\n",
       " 'õ': <gensim.models.keyedvectors.Vocab at 0x1a1c7adcf8>,\n",
       " '¶': <gensim.models.keyedvectors.Vocab at 0x1a1c7adb00>,\n",
       " 'ç': <gensim.models.keyedvectors.Vocab at 0x1a1c7adbe0>,\n",
       " 'è': <gensim.models.keyedvectors.Vocab at 0x1a1c7ad5c0>}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the vector for the word \"car\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.2399595e+00,  4.4667273e+00, -6.6916466e+00,  2.2073978e-01,\n",
       "       -1.6436852e+00, -1.3475132e+00, -1.9190580e+00,  2.0940568e-01,\n",
       "        1.4414124e-01,  1.5525151e+00,  1.0705235e+00,  2.7879739e+00,\n",
       "        1.0637932e+00,  3.5081275e+00,  2.3833985e+00,  8.1342614e-01,\n",
       "        1.2456235e+00,  2.4622865e+00,  1.2947204e+00,  2.2588758e+00,\n",
       "        5.4068074e+00, -4.9648452e+00,  7.9263358e+00,  2.5158229e+00,\n",
       "       -1.4917257e+00, -5.3672738e+00,  6.6290438e-01, -9.1990948e-01,\n",
       "       -1.3859427e+00, -6.8792577e+00,  5.7693520e+00, -9.6983057e-01,\n",
       "        1.7074025e+00, -4.6037421e+00, -2.9828639e+00, -2.7895813e+00,\n",
       "       -3.4015281e+00,  3.4548180e+00,  3.3909395e+00, -4.9129562e+00,\n",
       "       -2.3820224e+00,  5.8003986e-01,  1.9061019e+00, -1.0115103e+00,\n",
       "        2.8051648e+00,  1.4328101e+00,  1.0386212e+00, -2.5434239e+00,\n",
       "        1.8753095e+00, -2.8087747e-01,  2.7124012e+00,  8.8634866e-01,\n",
       "        2.7381344e+00,  3.9270139e+00,  1.6616899e-01,  1.8055469e+00,\n",
       "        6.1075306e+00, -7.3293412e-01, -1.2677619e+00,  3.3875244e+00,\n",
       "       -2.0650245e-01,  3.0874872e+00,  5.7474029e-01, -7.6236540e-01,\n",
       "       -2.6809940e+00,  2.2483031e-01,  2.8740418e+00,  7.3891357e-03,\n",
       "       -1.7665201e+00,  2.8143039e+00, -4.1343265e+00,  1.6535854e+00,\n",
       "       -2.6086965e+00, -4.8408365e+00,  3.1278548e-01, -1.7887523e+00,\n",
       "       -2.5538287e+00,  2.0010619e+00, -3.5100386e-01, -2.7010398e+00,\n",
       "       -4.8770245e-02,  1.7437731e+00,  1.6688670e-01,  2.0088389e+00,\n",
       "       -4.0942435e+00, -1.5852202e+00, -2.4775310e+00,  2.5471079e-01,\n",
       "        5.5593452e+00, -1.6952120e+00,  2.0262601e+00,  1.3515958e+00,\n",
       "       -6.2252313e-01, -3.2275088e+00, -1.7002711e+00, -2.9711332e+00,\n",
       "       -3.7238564e+00, -1.3853338e+00,  6.5408647e-01,  2.1289561e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['car']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to look up words similar to the word \"car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('car.', 0.8005589246749878),\n",
       " ('car,', 0.7972849011421204),\n",
       " ('bike', 0.7240657210350037),\n",
       " ('honda', 0.7106955051422119),\n",
       " ('auto', 0.7088593244552612)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('car', topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the Word2Vec model to return the similarity between two words that are present in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7240657"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"car\",w2=\"bike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"car\",w2=\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2343207"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity(w1=\"car\",w2=\"hard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "Now let's have a look at the BoW model. In some situations, using BoW may be better than Word Embedding, for example:\n",
    "1. Building an baseline model. By using scikit-learn, you need just a few lines of code to build model.\n",
    "2. If your dataset is small and context is domain specific. Context is very domain specific means that you cannot find corresponding Vector from pre-trained word embedding models (GloVe, fastText etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some simple ways to build BoW models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Occurrences\n",
    "Counting word occurrences. The reason behind using this approach is that keywords or important signals occur repeatedly. So the number of occurrences can represent the importance of word. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example sentence\n",
    "doc = \"In the-state-of-art of the NLP field, Embedding is the \\\n",
    "success way to resolve text related problem and outperform \\\n",
    "Bag of Words ( BoW ). Indeed, BoW introduced limitations \\\n",
    "large feature dimension, sparse representation etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>to</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>success</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>state</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sparse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Count\n",
       "16       of      3\n",
       "26      the      3\n",
       "3       bow      2\n",
       "0       and      1\n",
       "28      way      1\n",
       "27       to      1\n",
       "25     text      1\n",
       "24  success      1\n",
       "23    state      1\n",
       "22   sparse      1"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a CountVectorizer object\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "# Transforms the data into a bag of words (sparse matrix)\n",
    "count_occurs = count_vec.fit_transform([doc])\n",
    "\n",
    "# Create a table to count occurrences of each word\n",
    "count_occur_df = pd.DataFrame((count, word) for word, count in \n",
    "                              zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\n",
    "count_occur_df.columns = ['Word', 'Count']\n",
    "count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "count_occur_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Count Occurrences\n",
    "Normalization can be applied to avoid model bias. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>way</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>to</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>success</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>state</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>sparse</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word     Count\n",
       "16       of  0.428571\n",
       "26      the  0.428571\n",
       "3       bow  0.285714\n",
       "0       and  0.142857\n",
       "28      way  0.142857\n",
       "27       to  0.142857\n",
       "25     text  0.142857\n",
       "24  success  0.142857\n",
       "23    state  0.142857\n",
       "22   sparse  0.142857"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer object\n",
    "norm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "\n",
    "# Transforms the data into a bag of words (sparse matrix)\n",
    "norm_count_occurs = norm_count_vec.fit_transform([doc])\n",
    "\n",
    "# Create a table to count occurrences of each word\n",
    "norm_count_occur_df = pd.DataFrame((count, word) for word, count in \n",
    "                                   zip(norm_count_occurs.toarray().tolist()[0], norm_count_vec.get_feature_names()))\n",
    "norm_count_occur_df.columns = ['Word', 'Count']\n",
    "norm_count_occur_df.sort_values('Count', ascending=False, inplace=True)\n",
    "norm_count_occur_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the two BoW models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Count Vectorizer------\n",
      "Accuracy: 0.8926 (+/- 0.0092)\n",
      "Using TF Vectorizer------\n",
      "Accuracy: 0.8056 (+/- 0.0179)\n",
      "Using TF-IDF Vectorizer------\n",
      "Accuracy: 0.8908 (+/- 0.0108)\n"
     ]
    }
   ],
   "source": [
    "print('Using Count Vectorizer:')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='count')\n",
    "\n",
    "print('\\nUsing TF Vectorizer:')\n",
    "model_pipeline = pipeline(x_train, y_train, mode='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
